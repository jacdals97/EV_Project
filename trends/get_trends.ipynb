{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/jacda/OneDrive - ITU/Documents/ITU/Data_in_the_wild/EV_Project\n"
     ]
    }
   ],
   "source": [
    "from pytrends.request import TrendReq\n",
    "import json\n",
    "import pandas as pd  \n",
    "import numpy as np \n",
    "import joypy\n",
    "import matplotlib.pyplot as plt\n",
    "from  matplotlib import cm\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the unofficial Google Trends API, pytrends only supported city based queries for the US we had to redefine the TrendReq class to include cities from other countries as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyTrendReq(TrendReq):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def interest_by_region(self, resolution='COUNTRY', inc_low_vol=False,\n",
    "                           inc_geo_code=False):\n",
    "        \"\"\"Request data from Google's Interest by Region section and return a dataframe\"\"\"\n",
    "\n",
    "        # make the request\n",
    "        region_payload = dict()\n",
    "\n",
    "        if self.geo == '': \n",
    "            self.interest_by_region_widget['request']['resolution'] = resolution \n",
    "        elif self.geo == 'US' and resolution in ['DMA', 'CITY', 'REGION']: #DMA only exists for US\n",
    "            self.interest_by_region_widget['request']['resolution'] = resolution \n",
    "        elif len(self.geo) == 2 and resolution in ['CITY', 'REGION']: #If not US\n",
    "            self.interest_by_region_widget['request']['resolution'] = resolution        \n",
    "\n",
    "        self.interest_by_region_widget['request'][\n",
    "            'includeLowSearchVolumeGeos'] = inc_low_vol\n",
    "\n",
    "        # convert to string as requests will mangle\n",
    "        region_payload['req'] = json.dumps(\n",
    "            self.interest_by_region_widget['request'])\n",
    "        region_payload['token'] = self.interest_by_region_widget['token']\n",
    "        region_payload['tz'] = self.tz\n",
    "\n",
    "        # parse returned json\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.INTEREST_BY_REGION_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=region_payload,\n",
    "        )\n",
    "        df = pd.DataFrame(req_json['default']['geoMapData'])\n",
    "        if (df.empty):\n",
    "            return df\n",
    "\n",
    "        # rename the column with the search keyword\n",
    "        df = df[['geoName', 'value']].set_index(\n",
    "            ['geoName']).sort_index()\n",
    "        # split list columns into seperate ones, remove brackets and split on comma\n",
    "        result_df = df['value'].apply(lambda x: pd.Series(\n",
    "            str(x).replace('[', '').replace(']', '').split(',')))\n",
    "        if inc_geo_code:\n",
    "            result_df['geoCode'] = df['geoCode']\n",
    "\n",
    "        # rename each column with its search term\n",
    "        for idx, kw in enumerate(self.kw_list):\n",
    "            result_df[kw] = result_df[idx].astype('int')\n",
    "            del result_df[idx]\n",
    "\n",
    "        return result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented two functions to generate the the timeframes for the queries, one for months and one for weeks. Since weeks are a bit asynchronous with how they fall within different years the weeks implementation will include a few dates from before or after the year specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "def getDateRangeFromWeek(p_year,p_week):\n",
    "\n",
    "    firstdayofweek = datetime.datetime.strptime(f'{p_year}-W{int(p_week )- 1}-1', \"%Y-W%W-%w\").date()\n",
    "    lastdayofweek = firstdayofweek + datetime.timedelta(days=6.9)\n",
    "    return firstdayofweek, lastdayofweek\n",
    "\n",
    "def getDateRangeFromMonth(p_year,p_month):\n",
    "\n",
    "    firstdayofweek = datetime.datetime.strptime(f'{int(p_year)}-{int(p_month)}-15', \"%Y-%m-%d\").date()\n",
    "    if int(p_month) == 12:\n",
    "        lastdayofweek = datetime.datetime.strptime(f'{int(p_year)+1}-{1}-15', \"%Y-%m-%d\").date()\n",
    "    else:\n",
    "        lastdayofweek = datetime.datetime.strptime(f'{int(p_year)}-{int(p_month)+1}-15', \"%Y-%m-%d\").date()\n",
    "    return firstdayofweek, lastdayofweek\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating class and testing that the implementation works\n",
    "\n",
    "We choose sports as our reference term given our analysis carried out in the file 'Stable_trends_timeseries.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrends = MyTrendReq(hl='en-US', tz=-60, timeout=(15,30), retries=3, backoff_factor=0.2)\n",
    "keywords = ['/m/06ntj', '/m/03nlf2w'] #specify category ID listed below\n",
    "\n",
    "# news /m/05jhg\n",
    "# EV '/m/03nlf2w'\n",
    "#movies /m/02vxn\t\n",
    "#weather /m/0866r\n",
    "#sport /m/06ntj\n",
    "#music /m/04rlf\n",
    "#weather forecst /m/0jp7j\n",
    "\n",
    "pytrends.build_payload(kw_list = keywords, geo=\"DK-84\", timeframe= '2017-01-01T00 2017-01-08T00', gprop='')\n",
    "df = pytrends.interest_by_region(resolution='CITY', inc_low_vol=True, inc_geo_code=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append queries to dataframe loop through list of years and weeks\n",
    "# Catch exceptions from the pytrends API when it fails print the exception\n",
    "trends = pd.DataFrame()\n",
    "DK_geos = {'Nordjylland':\"DK-81\", 'Hovedstaden':\"DK-84\", 'Midtjylland':\"DK-82\", 'Sj√¶lland':\"DK-85\", 'Syddanmark':\"DK-83\"}\n",
    "def get_trends(years, trends, interval = [i+1 for i in range(52)]):    \n",
    "    pl_df = pd.DataFrame()\n",
    "    for reg, key in DK_geos.items():\n",
    "        pytrends.build_payload(kw_list = keywords, geo= key, timeframe= '2017-01-01T00 2017-01-08T00',)\n",
    "        pl = pytrends.interest_by_region(resolution='CITY', inc_low_vol=True, inc_geo_code=False)\n",
    "        pl.index.name = 'City'\n",
    "        pl.reset_index(inplace=True)\n",
    "        pl['Region'] = reg\n",
    "        pl_df = pl_df.append(pl)\n",
    "    pl_df = pl_df[['City', 'Region']]\n",
    "    if isinstance(years, int):\n",
    "        years = [years]\n",
    "    for year in years:    \n",
    "        for week in interval:\n",
    "            print(week)\n",
    "            reg_df = pd.DataFrame()\n",
    "            for reg, key in DK_geos.items():\n",
    "                try:\n",
    "                    firstdate, lastdate =  getDateRangeFromWeek(str(year),str(week))\n",
    "                    inter = str(firstdate)+'T00 ' + str(lastdate)+'T00'\n",
    "                    pytrends.build_payload(kw_list = keywords, geo= key, timeframe= inter)\n",
    "                    df = pytrends.interest_by_region(resolution='CITY', inc_low_vol=True, inc_geo_code=False)\n",
    "                    df.index.name = 'City'\n",
    "                    df.reset_index(inplace=True)\n",
    "                    df['Region'] = reg\n",
    "                    df['included'] = 1\n",
    "                    reg_df = reg_df.append(df)\n",
    "                except Exception as e:\n",
    "                    print(repr(e))\n",
    "                    df = pl_df[pl_df['Region']==reg]\n",
    "                    df['included'] = 1\n",
    "                    reg_df = reg_df.append(df)\n",
    "                    continue\n",
    "            df = pd.merge(pl_df, reg_df, how = 'left')\n",
    "            df['interval'] = inter\n",
    "            df['week'] = week\n",
    "            df['year'] = year\n",
    "            trends = trends.append(df) \n",
    "        time.sleep(60)\n",
    "    return trends\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code collects the Google Trends data. The time it takes to execute varies between the choice of keyterms. We first collect the data for all years between 2017 and 2020. Second we specify 2021 until week 45.\n",
    "\n",
    "The API will sometimes throw the error 500, which seems to be a server-side error from Google. At least other users have encountered the same issue in which queries break.\n",
    "\n",
    "If the API returns error code 429 it means that Google is stopping you from querying. Using a VPN solves this problem efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_trends = get_trends([2017,2018,2019,2020], trends)\n",
    "car_trends = get_trends([2021], car_trends, interval=[i+1 for i in range(48)]) #set to current week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_trends = car_trends.rename(columns={\"/m/03nlf2w\": \"Elbil\", \"/m/06ntj\": \"Sports\"})\n",
    "car_trends.to_csv('trends/sports_trends_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90 91 92 93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16450/3174865577.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trends_df['City'][trends_df['Unnamed: 0.1'].isin(Vallensb√¶k_ind[[0,2]])] = 'Vallensb√¶k Strand' #Vallensb√¶k Strand was called Vallensb√¶k\n"
     ]
    }
   ],
   "source": [
    "trends_df = pd.read_csv('trends/sports_trends_f.csv')\n",
    "\n",
    "names = ['City', 'Region', 'Sports', 'Elbil', 'included',\n",
    "       'interval', 'week', 'year']\n",
    "\n",
    "Vallensb√¶k_ind = trends_df[trends_df['City'] == 'Vallensb√¶k']['Unnamed: 0.1'].unique()\n",
    "print(Vallensb√¶k_ind)\n",
    "\n",
    "trends_df['City'][trends_df['Unnamed: 0.1'].isin(Vallensb√¶k_ind[[0,2]])] = 'Vallensb√¶k Strand' #Vallensb√¶k Strand was called Vallensb√¶k\n",
    "trends_df = trends_df.drop(trends_df[trends_df['Unnamed: 0.1'].isin(Vallensb√¶k_ind[[2,3]])].index) #It also contained duplicates\n",
    "\n",
    "trends_df = trends_df.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis = 1)\n",
    "\n",
    "trends_df.to_csv('Datasets/src/trends_data.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a18b077e14f581b8be01659f9df486d7c1e6db2b6bbdcf2e3e6431d172e3f10c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('stats': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
